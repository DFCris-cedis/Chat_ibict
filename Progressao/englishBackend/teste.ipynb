{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\Chat_ibict\\Progressao\\englishBackend\\teste.ipynb Cell 1\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/Chat_ibict/Progressao/englishBackend/teste.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mUsers\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmilen\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDocumentos\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mGitHub\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mChat_ibict\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mProgressao\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39menglishBackend\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/Chat_ibict/Progressao/englishBackend/teste.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#C:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\Chat_ibict\\Progressao\\englishBackend\\vectorial_model.py\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/Chat_ibict/Progressao/englishBackend/teste.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvectorial_model\u001b[39;00m \u001b[39mimport\u001b[39;00m get_vectorial_model, pre_processing\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/Chat_ibict/Progressao/englishBackend/teste.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mk_means\u001b[39;00m \u001b[39mimport\u001b[39;00m get_clusters\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/milen/OneDrive/Documentos/GitHub/Chat_ibict/Progressao/englishBackend/teste.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msentence_transformer\u001b[39;00m \u001b[39mimport\u001b[39;00m set_embedding\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "import json\n",
    "from fastapi import FastAPI\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\milen\\\\OneDrive\\\\Documentos\\\\GitHub\\\\Chat_ibict\\\\Progressao\\\\englishBackend\\\\')\n",
    "#C:\\Users\\milen\\OneDrive\\Documentos\\GitHub\\Chat_ibict\\Progressao\\englishBackend\\vectorial_model.py\n",
    "from .vectorial_model import get_vectorial_model, pre_processing\n",
    "from .k_means import get_clusters\n",
    "from .sentence_transformer import set_embedding\n",
    "import requests\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "import paramiko\n",
    "import boto3\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# get your instance ID from AWS dashboard\n",
    "\n",
    "class Data(BaseModel):\n",
    "    abstract: str\n",
    "    title: str\n",
    "\n",
    "def is_works(content):\n",
    "    if \"doi\" in content:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_remote_works(title, abstract):\n",
    "\n",
    "    print('entrando')\n",
    "\n",
    "    payload_dict = {\"title\":[title], \"abstract\": abstract, \"inverted_abstract\":False, \"journal\":\"\", \"doc_type\":\"\"}\n",
    "    r2 = requests.post('http://15.228.87.227:8080/invocations', json=payload_dict)\n",
    "    tags = json.loads(r2.text)[0]['tags']\n",
    "\n",
    "    return tags\n",
    "\n",
    "@app.get(\"/test\")\n",
    "def test_route():\n",
    "    # Dados de teste\n",
    "    test_data = Data(title=\"Teste de Título\", abstract=\"Teste de resumo\")\n",
    "\n",
    "    # Chama a função com os dados de teste\n",
    "    result = get_works(test_data)\n",
    "\n",
    "    # Retorna o resultado\n",
    "    return {\"Resultado do Teste\": result}\n",
    "\n",
    "@app.post(\"/works\")\n",
    "def get_works(data: Data):\n",
    "    print(data.abstract)\n",
    "    print(data.title)\n",
    "    concepts = get_remote_works(data.title, data.abstract)\n",
    "    return get_concepts_embeddings(concepts)\n",
    "\n",
    "def get_concepts_cluster(abstract):\n",
    "    phrases = []\n",
    "    journals = {}\n",
    "    # journals_count = {}\n",
    "    acc = 1\n",
    "    with open(\"./works.txt\",'r') as data_file:\n",
    "        for line in data_file:\n",
    "            work_words = ''\n",
    "            # print(line)\n",
    "            work = json.loads(line)\n",
    "            # print(work)\n",
    "            if work['concepts'] is not None:\n",
    "                for concept in work['concepts']:\n",
    "                    work_words = work_words + concept['display_name'] + ' '\n",
    "                journals[acc] = work['primary_location']\n",
    "                acc += 1\n",
    "                phrases.append(work_words)\n",
    "\n",
    "    get_clusters(phrases, abstract, journals)\n",
    "\n",
    "def get_abstracts_clusters(abstract):\n",
    "    print(abstract)\n",
    "\n",
    "    # abstract_keywords = abstract.split(' ')\n",
    "\n",
    "    phrases = []\n",
    "    journals = {}\n",
    "    journals_count = {}\n",
    "    acc = 1\n",
    "    with open(\"./works.txt\",'r') as data_file:\n",
    "        for line in data_file:\n",
    "            work_words = ''\n",
    "            # print(line)\n",
    "            work = json.loads(line)\n",
    "            # print(work)\n",
    "            if work['abstract_inverted_index'] is not None:\n",
    "                for word in work['abstract_inverted_index']:\n",
    "                    qty = len(work['abstract_inverted_index'][word])\n",
    "                    work_words = work_words + word + ' '\n",
    "                journals[acc] = work['primary_location']\n",
    "                acc += 1\n",
    "                phrases.append(work_words)\n",
    "\n",
    "    # print(phrases)\n",
    "    get_clusters(phrases, abstract, journals)\n",
    "    # set_embedding([abstract], phrases)\n",
    "\n",
    "def get_abstracts_embeddings(abstract):\n",
    "    print(abstract)\n",
    "\n",
    "    # abstract_keywords = abstract.split(' ')\n",
    "\n",
    "    phrases = []\n",
    "    journals = {}\n",
    "    journals_count = {}\n",
    "    acc = 0\n",
    "    with open(\"./works.txt\",'r') as data_file:\n",
    "        for line in data_file:\n",
    "            work_words = ''\n",
    "            # print(line)\n",
    "            work = json.loads(line)\n",
    "            # print(work)\n",
    "            if work['abstract_inverted_index'] is not None:\n",
    "                for word in work['abstract_inverted_index']:\n",
    "                    qty = len(work['abstract_inverted_index'][word])\n",
    "                    work_words = work_words + word + ' '\n",
    "                journals[str(acc)] = work['primary_location']\n",
    "                acc += 1\n",
    "                phrases.append(work_words)\n",
    "\n",
    "    final_ranking = set_embedding([abstract], phrases)\n",
    "    final_ranking = final_ranking[:1000]\n",
    "\n",
    "    print(journals)\n",
    "    \n",
    "    for index in final_ranking:\n",
    "        journal_id = journals[index]\n",
    "        print(journal_id)\n",
    "        if journal_id in journals_count:\n",
    "            journals_count[journal_id] += 1\n",
    "        else:\n",
    "            journals_count[journal_id] = 1\n",
    "\n",
    "    biggest_journals = sorted(journals_count, key=journals_count.get, reverse=True)[:3]\n",
    "\n",
    "    for i,journal in enumerate(biggest_journals):\n",
    "        id = journal.split('https://openalex.org/')[1]\n",
    "        url = 'https://api.openalex.org/sources/' + id\n",
    "        res = requests.get(url)\n",
    "        response = json.loads(res.text)\n",
    "        biggest_journals[i] = response['display_name']\n",
    "\n",
    "    return biggest_journals\n",
    "\n",
    "def get_concepts_embeddings(concepts):\n",
    "\n",
    "    print('Concepts Embeddings: ')\n",
    "\n",
    "    phrases = []\n",
    "    journals = {}\n",
    "    journals_count = {}\n",
    "    acc = 0\n",
    "    with open(\"./works.txt\",'r') as data_file:\n",
    "        for line in data_file:\n",
    "            work_words = ''\n",
    "            # print(line)\n",
    "            work = json.loads(line)\n",
    "            # print(work)\n",
    "            if work['concepts'] is not None:\n",
    "                for concept in work['concepts']:\n",
    "                    work_words = work_words + concept['display_name'] + ' '\n",
    "                journals[acc] = work['primary_location']\n",
    "                acc += 1\n",
    "                phrases.append(work_words)\n",
    "\n",
    "    final_ranking = set_embedding(concepts, phrases)\n",
    "    final_ranking = final_ranking[:1000]\n",
    "\n",
    "    print(journals)\n",
    "    \n",
    "    for index in final_ranking:\n",
    "        journal_id = journals[int(index)]\n",
    "        print(journal_id)\n",
    "        if journal_id in journals_count:\n",
    "            journals_count[journal_id] += 1\n",
    "        else:\n",
    "            journals_count[journal_id] = 1\n",
    "\n",
    "    biggest_journals = sorted(journals_count, key=journals_count.get, reverse=True)[:10]\n",
    "\n",
    "    for i,journal in enumerate(biggest_journals):\n",
    "        id = journal.split('https://openalex.org/')[1]\n",
    "        url = 'https://api.openalex.org/sources/' + id\n",
    "        res = requests.get(url)\n",
    "        response = json.loads(res.text)\n",
    "        biggest_journals[i] = response['display_name']\n",
    "\n",
    "    return biggest_journals\n",
    "\n",
    "def get_works_concepts(abstract, works):\n",
    "\n",
    "    abstract_keywords = abstract.split(' ')\n",
    "\n",
    "    phrases = []\n",
    "    journals = {}\n",
    "    journals_count = {}\n",
    "    acc = 1\n",
    "    for line in works:\n",
    "        work_words = []\n",
    "        work = json.loads(line)\n",
    "        print(work)\n",
    "        if work['abstract_inverted_index'] is not None:\n",
    "            for word in work['abstract_inverted_index']:\n",
    "                qty = len(work['abstract_inverted_index'][word])\n",
    "                work_words = work_words + ([word] * qty)\n",
    "            journals[acc] = work['primary_location']['source']['id']\n",
    "            acc += 1\n",
    "            phrases.append(work_words)\n",
    "\n",
    "    print(journals)\n",
    "\n",
    "    final_ranking = get_vectorial_model(phrases,abstract_keywords)\n",
    "    final_ranking = final_ranking[:1000]\n",
    "    \n",
    "    for index in final_ranking:\n",
    "        journal_id = journals[index]\n",
    "        if journal_id in journals_count:\n",
    "            journals_count[journal_id] += 1\n",
    "        else:\n",
    "            journals_count[journal_id] = 1\n",
    "\n",
    "    biggest_journals = sorted(journals_count, key=journals_count.get, reverse=True)[:3]\n",
    "\n",
    "    for i,journal in enumerate(biggest_journals):\n",
    "        id = journal.split('https://openalex.org/')[1]\n",
    "        url = 'https://api.openalex.org/sources/' + id\n",
    "        res = requests.get(url)\n",
    "        response = json.loads(res.text)\n",
    "        biggest_journals[i] = response['display_name']\n",
    "\n",
    "    return biggest_journals\n",
    "\n",
    "@app.get(\"/countries\")\n",
    "def read_root():\n",
    "    countries = [\n",
    "        'Brasil',\n",
    "        'Estados Unidos',\n",
    "        'Colombia',\n",
    "        'Uruguai'\n",
    "    ]\n",
    "\n",
    "    return countries\n",
    "\n",
    "def get_main_concepts():\n",
    "    # with open('./concepts.json') as json_data:\n",
    "    #     data = json.load(json_data)\n",
    "    #     df = pd.DataFrame(data['results'])\n",
    "    #     print(df.to_string())\n",
    "    #     concepts = df.to_xarray()\n",
    "    #     print(concepts)\n",
    "        # for concept in concepts:\n",
    "        #     print(concept)\n",
    "    concepts = [\n",
    "        \"Computer science\",\n",
    "        \"Medicine\",\n",
    "        \"Biology\",\n",
    "        \"Physics\",\n",
    "        \"Political science\",\n",
    "        \"Chemistry\",\n",
    "        \"Philosophy\",\n",
    "        \"Engineering\",\n",
    "        \"Mathematics\",\n",
    "        \"Psychology\",\n",
    "        \"Materials science\",\n",
    "        \"Art\",\n",
    "        \"Geography\",\n",
    "        \"Business\",\n",
    "        \"Sociology\",\n",
    "        \"Economics\",\n",
    "        \"Geology\",\n",
    "        \"Environmental science\"\n",
    "    ]\n",
    "\n",
    "    return concepts\n",
    "\n",
    "\n",
    "def get_category():\n",
    "    values = []\n",
    "    concepts = get_main_concepts()\n",
    "    concepts_works = {}\n",
    "\n",
    "    for c in concepts:\n",
    "        concepts_works[c] = []\n",
    "\n",
    "    print(concepts_works)\n",
    "\n",
    "    with open(\"./updated_date=2023-02-07/part_000.txt\",'r') as data_file:\n",
    "        # data_file2 = [data_file[0]]\n",
    "        # print(data_file[0])\n",
    "        for line in data_file:\n",
    "            concepts_in_sources = [0] * len(concepts)\n",
    "            source = json.loads(line)\n",
    "            # big_qty = [\"\",-1]\n",
    "            for concept in source['x_concepts']:\n",
    "                # print(concept)\n",
    "                if concept['display_name'] in concepts:\n",
    "                    index = concepts.index(concept['display_name'])\n",
    "                    concepts_in_sources[index] = concept['score']\n",
    "                # if concept['score'] > big_qty[1]:\n",
    "                #     big_qty = [concept['display_name'],concept['score']]\n",
    "                # elif concept['score'] == big_qty[1]:\n",
    "                #     source['category'] = None\n",
    "                #     big_qty[0] = None\n",
    "                #     break\n",
    "            # if big_qty[0] is not None:\n",
    "            source['category'] = concepts_in_sources\n",
    "            print(source)\n",
    "            \n",
    "            # print(source)\n",
    "            values.append(str(source))\n",
    "            break\n",
    "        data_file.close()\n",
    "\n",
    "    # print(values)\n",
    "\n",
    "    # with open(\"./updated_date=2023-02-07/part_000.txt\",'w') as data_file:\n",
    "    #     data_file.writelines(values)\n",
    "    #     data_file.close()\n",
    "    \n",
    "    # return values\n",
    "\n",
    "@app.get(\"/sources\")\n",
    "def read_root():\n",
    "    # get_category()\n",
    "    # with open(\"./updated_date=2023-02-07/part_000.txt\",'r') as data_file:\n",
    "    #     for line in data_file:\n",
    "    #         print(line)\n",
    "\n",
    "    return [{\"id\": 1234, \"name\": \"test\"}]\n",
    "\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: Union[str, None] = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
